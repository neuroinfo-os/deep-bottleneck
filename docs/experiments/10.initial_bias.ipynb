{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of different initial bias settings for relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "from deep_bottleneck.eval_tools.experiment_loader import ExperimentLoader\n",
    "from deep_bottleneck.eval_tools.utils import format_config, find_differing_config_keys\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = ExperimentLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_ids = [862, 859, 860, 868, 867, 863, 864, 866, 861, 865, 869, 870]\n",
    "experiments = loader.find_by_ids(experiment_ids)\n",
    "differing_config_keys = find_differing_config_keys(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_config_dict = {k: '<var>' for k in differing_config_keys}\n",
    "config = experiment.config\n",
    "config.update(variable_config_dict)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiment we varied the initial bias parameter of the hidden layers with an otherwise standard setting of the parameters under a `relu` activation function. Varying the initial bias it interesting, as this shifts the preactivation of the layers by the specified bias `output = activation(dot(input, kernel) + bias)`. This, in turn, affects the proportion of preactivations that end up in the saturation regime of the respective activation function. We hypothesize therefore that the inital bias can have an effect on the compression of a layer. In the case of `relu`, when the `initial_bias=0`, approximately half of the preactivations are negative and mapped to `0`. This induces \"immediate\" compression, as it was discussed for example in notebook `9.analyze_entropy`. By shifting the preactivations with the (positive) initial_bias parameter, we can supress immediate compression. Below the informationplane plots for different settings of the initial bias parameter are shown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(6,2, figsize=(14, 34))\n",
    "ax = ax.flat\n",
    "\n",
    "for i, experiment in enumerate(experiments):\n",
    "    img = plt.imread(BytesIO(experiment.artifacts['infoplane_test'].content))\n",
    "    ax[i].axis('off')\n",
    "    ax[i].imshow(img)\n",
    "    ax[i].set_title(format_config(experiment.config, *differing_config_keys),\n",
    "                    fontsize=16)\n",
    "plt.tight_layout()    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The informationplane plots shows two qualitaively different patterns. For negative initial bias settings as well as for initial bias settings wth large positive magnitude all gradients remain 0 for the entire training process, therefore not changing any weights in the network. In this setting mutual information of the resepctive layers stays constant over the entire process. Here, we only look at informationplane plots that show a some dynamic of the mutual inforamtion values over time, with `inital_bias=0` as a reference. For the penultimate layer and `initial_bias=0.2, 1 and 2` we can observe a decrease in mutual information with the input in the later stages of training (from epoch 1000 onwards). Does this movement towards the upper left in the information plane correspond to the network gradually learning weights and biases which push activations towards the neagtive spectrum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(6,2, figsize=(14, 34))\n",
    "ax = ax.flat\n",
    "\n",
    "for i, experiment in enumerate(experiments):\n",
    "    img = plt.imread(BytesIO(experiment.artifacts['infoplane_train'].content))\n",
    "    ax[i].axis('off')\n",
    "    ax[i].imshow(img)\n",
    "    ax[i].set_title(format_config(experiment.config, *differing_config_keys),\n",
    "                    fontsize=16)\n",
    "plt.tight_layout()    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we will have a close look on the activations of experiment with `initial_bias=2`, experiment_id=865"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias2 = loader.find_by_id(865)\n",
    "bias2.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(16, 20))\n",
    "\n",
    "img = plt.imread(BytesIO(bias2.artifacts['activations_train'].content))\n",
    "ax.axis('off')\n",
    "ax.imshow(img)\n",
    "ax.set_title(format_config(bias2.config, *differing_config_keys),\n",
    "                fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias2_multiple = loader.find_by_id(875)\n",
    "bias2_multiple.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1, figsize=(12, 24))\n",
    "ax = ax.flat\n",
    "\n",
    "img_train = plt.imread(BytesIO(bias2_multiple.artifacts['infoplane_train'].content))\n",
    "ax[0].axis('off')\n",
    "ax[0].imshow(img_train)\n",
    "ax[0].set_title(format_config(bias2_multiple.config, *differing_config_keys),\n",
    "                fontsize=16)\n",
    "img_test = plt.imread(BytesIO(bias2_multiple.artifacts['infoplane_test'].content))\n",
    "ax[1].axis('off')\n",
    "ax[1].imshow(img_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary material\n",
    "\n",
    "Below we find plots indicating the development of means and standard deviation of the gradient, its signal to noise ratio as well as the norm of the weight vector for all layers over the course of training. Comparing plots for unconstrained vs. constrained weight vector, we can reassure ourselves that rescaling the weights worked as we expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(6,2, figsize=(12, 21))\n",
    "ax = ax.flat\n",
    "\n",
    "for i, experiment in enumerate(experiments): \n",
    "    df = pd.DataFrame(data=np.array([experiment.metrics['training.accuracy'].values, \n",
    "                                     experiment.metrics['test.accuracy'].values]).T,\n",
    "                  index=experiment.metrics['test.accuracy'].index,\n",
    "                  columns=['train_acc', 'val_acc'])\n",
    "\n",
    "    df.plot(linestyle='', marker='.', markersize=5, ax=ax[i])\n",
    "    ax[i].set_title(format_config(experiment.config, *differing_config_keys),\n",
    "                    fontsize=12)\n",
    "    ax[i].set_ylim([0,1])\n",
    "    ax[i].set(xlabel='epoch', ylabel='accuracy')\n",
    "\n",
    "plt.tight_layout()    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(12,1, figsize=(14, 34))\n",
    "ax = ax.flat\n",
    "\n",
    "for i, experiment in enumerate(experiments):\n",
    "    img = plt.imread(BytesIO(experiment.artifacts['snr_train'].content))\n",
    "    ax[i].axis('off')\n",
    "    ax[i].imshow(img)\n",
    "    ax[i].set_title(format_config(experiment.config, *differing_config_keys),\n",
    "                    fontsize=16)\n",
    "plt.tight_layout()    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
