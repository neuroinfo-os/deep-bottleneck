Literature summary
==================

1. The information bottleneck method (Tishby 1999)
--------------------------------------------------
:cite:`Tishby2000`



2. Deep learning and the information bottleneck principle (Tishby 2015)
-----------------------------------------------------------------------
:cite:`Tishby2015`



3. Opening the black box of deep neural networks via information (Tishby 2017)
------------------------------------------------------------------------------
:cite:`Schwartz-ziv2017`



4. On the information bottleneck theory of deep learning (Saxe 2018)
--------------------------------------------------------------------
:cite:`Saxe2018`



4.1 Key points of the paper
^^^^^^^^^^^^^^^^^^^^^^^^^^^

* none of the following claims of Tishby (:cite:`Tishby2015`) holds in the general case:

    #. deep networks undergo two distinct phases consisting of an initial fitting phase and a subsequent compression phase
    #. the compression phase is causally related to the excellent generalization performance of deep networks
    #. the compression phase occurs due to the diffusion-like behavior of stochastic gradient descent

* the observed compression is different based on the activation function: double-sided saturating nonlinearities like tanh
  yield a compression phase, but linear activation functions and single-sided saturating nonlinearities like ReLU do not.

* there is no evident causal connection between compression and generalization.

* the compression phase, when it exists, does not arise from stochasticity in training.

* when an input domain consists of a subset of task-relevant and task-irrelevant information, the task-irrelevant information compress
  although the overall information about the input may monotonically increase with training time. This compression happens concurrently
  with the fitting process rather than during a subsequent compression period.

4.2 Most important experiments
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
#. Tishby's experiment reconstructed:

    * 7 fully connected hidden layers of width 12-10-7-5-4-3-2
    * trained with stochastic gradient descent to produce a binary classification from a 12-dimensional input
    * 256 randomly selected samples per batch
    * mutual information is calculated by binning the output activations into 30 equal intervals between -1 and 1
    * trained on Tishby's dataset
    * tanh-activation function

#. Tishby's experiment reconstructed with ReLU activation:

    * 7 fully connected hidden layers of width 12-10-7-5-4-3-2
    * trained with stochastic gradient descent to produce a binary classification from a 12-dimensional input
    * 256 randomly selected samples per batch
    * mutual information is calculated by binning the output activations into 30 equal intervals between -1 and 1
    * ReLu-activation function

#. Tanh-activation function on MNIST:

    * 6 fully connected hidden layers of width 784 - 1024 - 20 - 20 - 20 - 10
    * trained with stochastic gradient descent to produce a binary classification from a 12-dimensional input
    * non-parametric kernel density mutual information estimator
    * trained on MNIST dataset
    * tanh-activation function

#. ReLU-activation function on MNIST:

    * 6 fully connected hidden layers of width 784 - 1024 - 20 - 20 - 20 - 10
    * trained with stochastic gradient descent to produce a binary classification from a 12-dimensional input
    * non-parametric kernel density mutual information estimator
    * trained on MNIST dataset
    * ReLU-activation function

4.3 Presentation
^^^^^^^^^^^^^^^^

`Click here to open presentation as PDF document. <_static/on_the_information_bottleneck_theory_presentation.pdf>`_


5. Estimating mutual information
--------------------------------
:cite:`Kraskov2004`


5.1 Introduction
^^^^^^^^^^^^^^^^
- Kraskov suggests an alternative mutual information estimator that is not based
  on binning but on k-nearest neighbour distances.

- Mutual information is often used as a measure of independence between random
  variables. We note that mutual information is zero if and only if two random
  variables are strictly independent.

- Mutual information has some well known properties and advantages since it has
  close ties to Shannon entropy (see appendix of the paper), still estimating mutual
  information is not always that easy.

- Most mutual information estimation techniques are based on binning, which often
  leads to a systematic error.

- Consider a set of :math:`N` bivariate measurements, :math:`z_i = (x_i, y_i),
  i = 1,...,N`, which are assumed to be iid (independent identically distributed)
  realizations of a random variable :math:`Z=(X,Y)` with density :math:`\mu (x,y)`.
  :math:`x` and :math:`y` can be scalars or elements of a higher dimensional space.

- For simplicity we say that :math:`0 \cdot \log(0) = 0` in order to consider probability
  density functions that do not have to be strictly positive.

- The marginal densities of :math:`X` and :math:`Y` can be denoted as follows:

  .. math::
     \mu_x(x) = \int \mu (x,y) dy \ \text{and } \ \mu_y(y) = \int \mu (x,y) dx.

- Therefore we can define mutual information as

  .. math::
     I(x,y) = \int_Y \int_X \mu (x,y) \cdot \log \dfrac{\mu (x,y)}{\mu_x (x) \mu_y(y)} dx dy.

- Note that the base of the logarithm sets the unit in which information is measured.
  That means that if we want to measure in bits, we have to take base 2. In the
  following we will take the natural logarithm for estimating mutual information.

- Our aim is to estimate mutual information without any knowledge of the probability
  functions :math:`\mu`, :math:`\mu_x` and :math:`\mu_y`. The only information we
  have is set :math:`\{ z_i \}`.


5.2 Binning
^^^^^^^^^^^
- Binning is an often used technique to estimate mutual information. Therefore we
  partition the supports of :math:`X` and :math:`Y` into bins of finite size by
  considering the finite sum:

  .. math::
     I(X,Y) \approx I_{\text{binned}} (X,Y) \equiv \sum_{i,j} p(i,j) \log \dfrac{p(i,j)}{p_x(i)p_y(j)},

  where :math:`p_x(i) = \int_i \mu_x (x) dx, p_y(j) = \int_j \mu_y(y)` and
  :math:`p(i,j) = \int_i \int_j \mu (x,y) dx dy` (meaning :math:`\int_i` is the
  integral over bin :math:`i`).

- Set :math:`n_x(i)` to be the number of points falling into bin i of :math:`X`
  and analogous to that set :math:`n_y(j)` to be the number of points falling into
  bin j of :math:`Y`. Moreover, :math:`n(i,j)` is the number of points in their
  intersection.

- Since we do not know the exact probability density function, we approximate them
  with :math:`p_x(i) \approx \frac{n_x(i)}{N}`, :math:`p_y(j) \approx \frac{n_y(j)}{N}`,
  and :math:`p(i,j) \approx \frac{n(i,j)}{N}`.

- For :math:`N \rightarrow \infty` and bin sizes tending to zero, the binning
  approximation (:math:`I_{\text{binned}}`) indeed converges to :math:`I(X,Y)`. Constraint: all
  densities exist as proper functions.

- Note that the bin size do not have to be the same for each bin. Adaptive bin sizes
  actually lead to much better estimations.

5.3 Kraskov estimator
^^^^^^^^^^^^^^^^^^^^^
- The Kraskov estimator uses k-nearest neighbour statistics to estimate mutual
  information.

- The basic idea is to estimate :math:`H(X)` from the average distance to the
  k-nearest neighbour, averaged over all :math:`x_i`.

- Since mutual information between two random variables can also be written as

  .. math::
     I(X,Y) = H(X) + H(Y) - H(X,Y),

  with :math:`H(X)= - \int \mu (x) \log \mu (x) dx` being the Shannon entropy, we
  can estimate the mutual information by estimating the Shannon entropy for
  :math:`H(X)`, :math:`H(Y)` and :math:`H(X,Y)`.
  This estimation would mean that the errors made in the individual estimates would
  presumably not cancel. Therefore, we proceed a bit differently:

- Assume some metrics to be given on the spaces by :math:`X, Y` and :math:`Z=(X,Y)`.

- For each point :math:`z_i=(x_i,y_i)` we rank its neighbours by distance
  :math:`d_{i,j} = ||z_i - z_j||: d_{i,j_1} \leq d_{i,j_2} \leq d_{i,j_3} \leq ...`.
  Similar rankings can be done in the subspaces :math:`X` and :math:`Y`.

- Furthermore, we will use the maximum norm for the distances in the space
  :math:`Z=(X,Y)`, i.e.

  .. math::
     ||z-z'||_{\max} = \max \{ ||x - x'||, ||y - y'||\},

  while any norms can be used for :math:`||x - x'||` and :math:`||y - y'||`.

- We make further notations: :math:`\frac{\epsilon (i)}{2}` is the distance between
  :math:`z_i` and its :math:`k`-th neighbour. :math:`\frac{\epsilon_x (i)}{2}` and
  :math:`\frac{\epsilon_y (i)}{2}` denote the distance between the same points projected
  into the :math:`X` and :math:`Y`subspaces.
- Note that :math:`\epsilon(i)=\max \lbrace frac{\epsilon_x (i)}{2}, frac{\epsilon_y (i)}{2}\rbrace`.
- In the following, two algorithms for estimating mutual information will be taken
  into account:

    #. In the **first algorithm**, the numbers of points :math:`x_j` whose distance from
       :math:`x_i` is strictly less than :math:`\frac{\epsilon (i)}{2}` is counted
       and called :math:`n_x(i)`. Analogous for :math:`y`.
    #. By :math:`<...>` the averages over all :math:`i \in [1,...,N]` and over all
       realisations of random samples is denoted:

       .. math::
       <...> = \dfrac{1}{N} \sum_{i=1}^N E[...(i)].

    #. The mutual information can then be estimated with:

       .. math::
       I^{(1)}(X,Y) = \psi (k) - <\psi (n_x + 1) + \psi (n_y + 1)> + \psi (N).

    #. In the **second algorithm** :math:`n_x(i)` and :math:`n_y(i)` are replaced
       by the number of points that satisfy the following equations:

       .. math::
       ||x_i - x_j|| \leq \dfrac{\epsilon_x (i)}{2} \text{and} ||y_i - y_j|| \leq \dfrac{\epsilon_y (i)}{2}

    #. Then mutual information can be estimated via

       ..math::
        I^{(2)}(X,Y) = \psi (k) - 1/k - <\psi (n_x) + \psi (n_y)> + \psi (N).

- Generally, both estimates give similar results. But it proves that :math:`I^{(1)}`
  has the tendency to have slightly smaller statistical errors, but larger
  systematic errors. This means that when we are interested in very high
  dimensions, we better should use :math:`I^{(2)}`.


6. SVCCA: singular vector canonical correlation analysis
--------------------------------------------------------
:cite:`Raghu2017`

6.1 Key points of the paper
^^^^^^^^^^^^^^^^^^^^^^^^^^^

- They developed a method that analyses each neuron's activation vector (i.e.
  the scalar outputs that are emitted on input data points). This analysis gives an
  insight into learning dynamics and learned representation.

- SVCCA is a general method that compares two learned representations of
  different neural network layers and architectures. It is either possible to
  compare the same layer at different time steps, or simply different layers.

- The comparison of two representations fulfills two important properties:

    * It is invariant to affine transformation (which allows the comparison
      between different layers and networks).

    * It is fast to compute, which allows more comparisons to be calculated
      than with previous methods.

6.2 Experiment set-up
^^^^^^^^^^^^^^^^^^^^^

- **Dataset**: mostly CIFAR-10 (augmented with random translations)

- **Architecture**: One convolutional network and one residual network

- In order to produce a few figures, they decided to design a toy regression task (training a four hidden layer fully connected network with 1D input and 4D output)


6.3 How SVCCA works
^^^^^^^^^^^^^^^^^^^

- SVCCA is short for Singular Vector Canonical Correlation Analysis and
  therefore combines the Singular Value Decomposition with a Canonical Correlation
  Analysis.

- The representation of a neuron is defined as a table/function that maps the
  inputs on all possible outputs for a single neuron. Its representation is
  therefore studied as a set of responses over a finite set of inputs. Formally,
  that means that given a dataset :math:`X = {x_1,...,x_m}` and a neuron :math:`i`
  on layer :math:`l`, we define :math:`z^{l}_{i}` to be the vector of outputs on
  :math:`X`, i.e.

    .. math::

      z^{l}_{i} = (z^{l}_{i}(x_1),··· ,z^{l}_{i}(x_m)).

  Note that :math:`z^{l}_{i}` is a single neuron's response over the entire
  dataset and not an entire layer's response for a single input. In this sense
  the neuron can be thought of as a single vector in a high-dimensional space.
  A layer is therefore a subspace of :math:`\mathbb{R}^m` spanned by its neurons'
  vectors.

1. **Input**: takes two (not necessarily different) sets of neurons (typically layers of a network)

    .. math::

      l_1 = {z^{l_1}_{1}, ..., z^{l_{m_1}}_{l_1}} \text{ and } l_2 = {z^{l_2}_{1}, ..., z^{l_{m_2}}_{l_2}}

2. **Step 1**: Use SVD of each  subspace to get sub-subspaces :math:`l_1' \in l_1` and :math:`l_2' \in l_2`, which contain of the most important directions of the original subspaces :math:`l_1, l_2`.

3. **Step 2**: Compute Canonical Correlation similarity of :math:`l_1', l_2'`: linearly transform :math:`l_1', l_2'` to be as aligned as possible and compute correlation coefficients.

4. **Output**: pairs of aligned directions :math:`(\widetilde{z}_{i}^{l_1}, \widetilde{z}_{i}^{l_2})` and how well their correlate :math:`\rho_i`. The SVCCA similarity is defined as

    .. math::
      \bar{\rho} = \frac{1}{\min(m_1,m_2)} \sum_i \rho_i .

6.4 Results
^^^^^^^^^^^

- The dimensionality of a layer's learned representation does not have to be the same number than the number of neurons in the layer.

- Because of a bottom up convergence of the deep learning dynamics, they suggest a computationally more efficient method for training the network - *Freeze Training*. In Freeze Training  layers are sequentially frozen after a certain number of time steps.

- Computational speed up is successfully done with a Discrete Fourier Transform causing all block matrices to be block-diagonal.

- Moreover, SVCCA captures the semantics of different classes, with similar classes having similar sensitivities, and vice versa.



.. bibliography:: references.bib
