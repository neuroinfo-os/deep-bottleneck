%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for Farina Kock at 2018-12-12 11:20:06 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@article{Kraskov2004,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {https://ui.adsabs.harvard.edu/\#abs/2004PhRvE..69f6138K},
	Archiveprefix = {arXiv},
	Author = {{Kraskov}, Alexander and {St{\"o}gbauer}, Harald and {Grassberger}, Peter},
	Date-Added = {2018-12-12 11:19:19 +0100},
	Date-Modified = {2018-12-12 11:20:05 +0100},
	Doi = {10.1103/PhysRevE.69.066138},
	Eid = {066138},
	Eprint = {cond-mat/0305641},
	Journal = {\pre},
	Keywords = {05.90.+m, 02.50.-r, 87.10.+e, Other topics in statistical physics thermodynamics and nonlinear dynamical systems, Probability theory stochastic processes and statistics, General theory and mathematical aspects, Condensed Matter - Statistical Mechanics, Condensed Matter - Disordered Systems and Neural Networks},
	Month = Jun,
	Pages = {066138},
	Primaryclass = {cond-mat.stat-mech},
	Title = {{Estimating mutual information}},
	Volume = {69},
	Year = 2004,
	Bdsk-Url-1 = {https://doi.org/10.1103/PhysRevE.69.066138}}

@inproceedings{Saxe2018,
	Abstract = {The practical successes of deep neural networks have not been matched by theoretical progress that satisfyingly explains their behavior. In this work, we study the information bottleneck (IB) theory of deep learning, which makes three specific claims: first, that deep networks undergo two distinct phases consisting of an initial fitting phase and a subsequent compression phase; second, that the compression phase is causally related to the excellent generalization performance of deep networks; and third, that the compression phase occurs due to the diffusion-like behavior of stochastic gradient descent. Here we show that none of these claims hold true in the general case. Through a combination of analytical results and simulation, we demonstrate that the information plane trajectory is predominantly a function of the neural nonlinearity employed: double-sided saturating nonlinearities like tanh yield a compression phase as neural activations enter the saturation regime, but linear activation functions and single-sided saturating nonlinearities like the widely used ReLU in fact do not. Moreover, we find that there is no evident causal connection between compression and generalization: networks that do not compress are still capable of generalization, and vice versa. Next, we show that the compression phase, when it exists, does not arise from stochasticity in training by demonstrating that we can replicate the IB findings using full batch gradient descent rather than stochastic gradient descent. Finally, we show that when an input domain consists of a subset of task-relevant and task-irrelevant information, hidden representations do compress the task-irrelevant information, although the overall information about the input may monotonically increase with training time, and that this compression happens concurrently with the fitting process rather than during a subsequent compression period.},
	Author = {Andrew Michael Saxe and Yamini Bansal and Joel Dapello and Madhu Advani and Artemy Kolchinsky and Brendan Daniel Tracey and David Daniel Cox},
	Booktitle = {International Conference on Learning Representations},
	Date-Added = {2018-10-31 11:05:58 +0100},
	Date-Modified = {2018-10-31 11:17:18 +0100},
	Title = {On the Information Bottleneck Theory of Deep Learning},
	Url = {https://openreview.net/forum?id=ry_WPG-A-},
	Year = {2018},
	Bdsk-Url-1 = {https://openreview.net/forum?id=ry_WPG-A-}}

@article{Tishby2015,
	Abstract = {Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain finite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network's simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.},
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150302406T},
	Archiveprefix = {arXiv},
	Author = {{Tishby}, N. and {Zaslavsky}, N.},
	Date-Added = {2018-10-31 10:58:16 +0100},
	Date-Modified = {2018-10-31 11:15:38 +0100},
	Eprint = {1503.02406},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Machine Learning},
	Month = mar,
	Title = {{Deep Learning and the Information Bottleneck Principle}},
	Year = 2015}

@article{Tishby2000,
	Abstract = {We define the relevant information in a signal xâˆˆX as being the information that this signal provides about another signal $y\in \Y$. Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal x requires more than just predicting y, it also requires specifying which features of $\X$ play a role in the prediction. We formalize this problem as that of finding a short code for $\X$ that preserves the maximum information about $\Y$. That is, we squeeze the information that $\X$ provides about $\Y$ through a `bottleneck' formed by a limited set of codewords $\tX$. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure $d(x,\x)$ emerges from the joint statistics of $\X$ and $\Y$. This approach yields an exact set of self consistent equations for the coding rules $X \to \tX$ and $\tX \to \Y$. Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut-Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere.},
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2000physics...4057T},
	Author = {{Tishby}, N. and {Pereira}, F.~C. and {Bialek}, W.},
	Date-Added = {2018-10-31 10:54:39 +0100},
	Date-Modified = {2018-10-31 11:20:25 +0100},
	Eprint = {physics/0004057},
	Journal = {ArXiv Physics e-prints},
	Keywords = {Physics - Data Analysis, Statistics and Probability, Condensed Matter - Disordered Systems and Neural Networks, Computer Science - Machine Learning, Nonlinear Sciences - Adaptation and Self-Organizing Systems},
	Month = apr,
	Title = {{The information bottleneck method}},
	Year = 2000}

@article{Raghu2017,
	Abstract = {We propose a new technique, Singular Vector Canonical Correlation Analysis (SVCCA), a tool for quickly comparing two representations in a way that is both invariant to affine transform (allowing comparison between different layers and networks) and fast to compute (allowing more comparisons to be calculated than with previous methods). We deploy this tool to measure the intrinsic dimensionality of layers, showing in some cases needless over-parameterization; to probe learning dynamics throughout training, finding that networks converge to final representations from the bottom up; to show where class-specific information in networks is formed; and to suggest new training regimes that simultaneously save computation and overfit less.},
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170605806R},
	Archiveprefix = {arXiv},
	Author = {{Raghu}, M. and {Gilmer}, J. and {Yosinski}, J. and {Sohl-Dickstein}, J.},
	Date-Added = {2018-10-31 10:35:13 +0100},
	Date-Modified = {2018-10-31 11:18:46 +0100},
	Eprint = {1706.05806},
	Journal = {ArXiv e-prints},
	Keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	Month = jun,
	Primaryclass = {stat.ML},
	Title = {{SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability}},
	Year = 2017}

@inproceedings{Schwartz-ziv2017,
	Abstract = {Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work [Tishby and Za-slavsky (2015)] proposed to analyze DNNs in the Information Plane; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compres-sion and prediction, successively, for each layer. In this work we follow up on this idea and demonstrate the effectiveness of the Information-Plane visualization of DNNs. Our main results are: (i) most of the training epochs in standard DL are spent on compression of the input to efficient representation and not on fitting the training labels. (ii) The representation compression phase begins when the training errors becomes small and the Stochastic Gradient Decent (SGD) epochs change from a fast drift to smaller training error into a stochastic relaxation, or random diffusion, constrained by the training error value. (iii) The converged layers lie on or very close to the Information Bottleneck (IB) theoretical bound, and the maps from the input to any hidden layer and from this hidden layer to the output satisfy the IB self-consistent equations. This generalization through noise mechanism is unique to Deep Neural Networks and absent in one layer networks. (iv) The training time is dramatically reduced when adding more hidden layers. Thus the main advantage of the hidden layers is computational. This can be explained by the reduced relaxation time, as this it scales super-linearly (exponentially for simple diffusion) with the information compression from the previous layer. (v) As we expect critical slowing down of the stochastic relaxation near phase transitions on the IB curve, we expect the hidden layers to converge to such critical points.},
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1703.00810v3},
	Author = {Schwartz-ziv, Ravid and Tishby, Naftali},
	Booktitle = {Arxiv},
	Eprint = {arXiv:1703.00810v3},
	File = {::},
	Keywords = {Deep Learning,Deep Neural Networks,Information Bottleneck,Representation Learning},
	Pages = {1--19},
	Title = {{Opening the black box of Deep Neural Networks via Information}},
	Url = {https://arxiv.org/pdf/1703.00810.pdf},
	Year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1703.00810.pdf}}
